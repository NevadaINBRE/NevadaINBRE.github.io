---
title: 'Microbiome Workshop'
output: html_document
---

## Microbiome Analysis

Install necessary packages:

```{r}
#pacman is a package that loads and installs packages 
library(pacman)
p_load(tidyverse, dada2, decontam, phyloseq, Biostrings, vegan)
# I always set a seed in case any of my functions rely on random values and i want reproducibility.
set.seed(3618)
```
### Download data

Download the raw fastqs and metadata here and put in your project folder 

```{r}
#set this to the path to the raw files
path <- "~/Documents/El_Bryan_NSC/01_rawfastq/NSC_Workshop"
#Save file names to a list
fnFs <- sort(list.files(path, pattern="_R1_001*", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001*", full.names = TRUE))
head(fnFs)
```
Next we need to create a list of the sample names. This code will be different depending on how the fastqs you have are names. In this case our files are named EH_BS_16SPool_S1_L001_RX_001.SAMPLENAME.fastq. A lot of sequencing centers will give back demultiplexed samples with the format: SAMPLENAME_L001_RX_001.fastq. The commented out code gives and an example of how to extract those samples names
 
```{r}
# Extract sample names, filenames have format: NeedsToBeCut_XXX.SAMPLENAME.fastq
sample.names <- sapply(strsplit(basename(fnFs), "\\."), `[`, 2)
# SAMPLENAME_L001_RX_001.fastq extraction
#sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

Next we are going to visualize the quality profiles of the forward and reverse reads. This function produces a gray-scale heatmap of the frequencies of quality score. Usually the reverse reads drop off in quality at the end, but we want the mean quality (green line) and quartiles (orange line) to be above 30. 

```{r}
plotQualityProfile(fnFs[1:2]) 
plotQualityProfile(fnRs[1:2])
```
## Filter and Trim

The next step is trimming and filtering. First we are going to create the filenames where the filtered reads are going to exist. Then we are going to run the `filterAndTrim` function. There are a lot of arguments for this function but the main ones we are going to change based on the setup of our  are `truncLen`, `multithread` and `trimLeft`. We want the our forward and reverse reads to have at least 20 bp overlap and should trim from both sides. `trimLeft` cuts off the primers that are being used. Given that 16S rRNA primer usually use degenerate codes (Y,M,W, etc.), it improves the quality to cut off basepairs of the primers. In our data, 515F (19bp) and 806R (20bp) primers were used which produces a 290 bp product. The reads are 250 bp paired end reads. If you are running on a windows machine, you want to set `multithread`=FALSE

```{r} 
# ~20 min to run
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
# Truncating at 200 bp for forward and 120 bp for reverse 
#500-290= 210bp of overlap. 290-120(r)=170, 170+30=200 (f)
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(200,120), maxN=0, maxEE=c(2,2), trimLeft = c(19,20), truncQ=2, rm.phix=TRUE, compress=TRUE) 
head(out)
```

`maxN` is a requirement of DADA2 that Ns are filtered out. `rm.phix` removes the phiX reads that are spiked in to increase the bp diversity. `maxEE` sets the maximum number of expected errors allowed in a read. 

## Learning Error Rates 

Every sequencing run has a different set of error rates and the DADA2  algorithm uses a parametric error model to learn error rates. The `plotErrors` function  can be used as a quality assurance method. The points show the observed error rates, the black line shows the estimated error rates after convergence of the machine learning algorithm and the red line shows the expected error rates under nominal definition of the Q-score. 

```{r}
#This step takes a long time so instead we are going to load the already generated files
# errF <- learnErrors(filtFs, multithread=TRUE)
# errR <- learnErrors(filtRs, multithread=TRUE)
errR<-readRDS(file = paste0(path,"/errR.rds"))
errF<-readRDS(file = paste0(path,"/errF.rds"))
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ = T)
```

## Amplicon Sequence Variants (ASV) Inference 

Next the core sample inference algorithm is applied to the trimmed and filtered data. You can read more about the algorithm here: https://www.nature.com/articles/nmeth.3869#methods. After unique sequences are identified the forward and reverse reads are merged. We have saved the tabulated sequence table as an R object that you can view. 

```{r}
#These steps also take a long time so we are going load the R object of tabulated sequences instead
# dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
# dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
# mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# seqtab <- makeSequenceTable(mergers)

seqtab<-readRDS("seqtab.rds")
seqtab[1:2,1:2]
```
From the results we can see that the seq table lists the sequence variant for the columns and the samples for the rows. The numbers of reads make up the matrix. One important aspect of DADA2 is that instead of naming ASVs with a placeholder ("ASV1" "ASV2", etc.), it keeps the sequence variant so multiple sequencing runs can be combined. 


## Chimera removal 
From the DADA2 tutorial: 

>The core `dada` method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. 

We can also calculated the amount of chimeras removed. Chimeras can make up a proportion of the merged variants but usually should not make up a large proportion of the merged reads. In our case they make up ~6% of the reads which is fine to continue. 

```{r}

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
#saveRDS(seqtab.nochim, "seqtabnochim.rds")
1-sum(seqtab.nochim)/sum(seqtab) 
```

## Tracking reads 

As a quality assurance  measure, we can make a data frame of the number of reads in each step: 

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
view(track)
```

## Assigning Taxonomy

DADA2 includes a function `assignTaxonomy` that implements a naive Bayesian classifier method. This function uses the ASV sequences and a training set of reference sequences to assign taxonomy. You can use the argument `minBoot` to adjust the minimum bootstrap confidence needed for assignment. The `assignTaxonomy` function only assigns down to genus level which is usually sufficient for most analyses. However, if you are interested in looking at the species level you are able to do that using 100% sequence identity and the `addSpecies` function. 

There are 3 different trained reference sets formatted for DADA2 available on their website:
  - Silva
  - RDP
  - Greengenes
You can download the Silva data here: https://benjjneb.github.io/dada2/training.html

```{r}
#change refFatsta to where you downloaded the silva data
taxa<- assignTaxonomy(seqs = seqtab.nochim, refFasta = "~/Documents/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)
taxa[1,1:6]
#saveRDS(object = taxa,"taxa.rds")
```
## Contaminants

We can use `decontam` to remove statistically significant contaminants using the controls. 
```{r}
metadata<-suppressMessages(read_tsv("Metadata.txt"))%>%
  column_to_rownames(., var="Name")
head(metadata)
control<-grepl("Control",metadata$Sample)
contam<- isContaminant(seqtab.nochim, method= "prevalence", neg=control)
contam_asvs <- row.names(contam[contam$contaminant == TRUE, ])
table(contam$contaminant)#30 contaminants
# removing contaminants from taxa
taxa_no_contam<-taxa[!rownames(taxa)%in% contam_asvs,]
dim(taxa)- dim(taxa_no_contam)
seqtab.nocontam<-seqtab.nochim[, !colnames(seqtab.nochim) %in% contam_asvs]
dim(seqtab.nochim)-dim(seqtab.nocontam)
```

## Phyloseq Object

Now that we have created the sequence tab and the taxonomy table, we can move to downstream analysis. There are many different ways to explore microbiomes including dimension reductions (NMDS, PCA, UMAP, etc.), relative abundance bar charts, and differential abundance. Tehre are many different packages you can use including `vegan`, baseR,'decontam' `EasyMicroPlot`, etc., but today we are going to go over how to use `Phyloseq`. First we need to transfer our data to the phyloseq object. 

```{r}

ps <- phyloseq(otu_table(seqtab.nocontam, taxa_are_rows=FALSE), 
               tax_table(taxa_no_contam), 
               sample_data(metadata))

# Since we are only analyzing this dataset and not adding more runs it makes sense to change the ASV names from sequences to shorter nicknames
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
#refseq is now a table of nickname and sequence
#saveRDS(object=ps, "phyloseq.rds")
```


## Plots in Phyloseq

We can use the pre-loaded functions in phyloseq to subset the data and plot diversity measures.

```{r}
theme_set(theme_classic())
#Removing the controls from the phyloseq
ps_no_control<-subset_samples(ps, Sample=="Experiment")
#Alpha diversity
plot_richness(ps_no_control, x="Water_Temperature_C", measures=c("Shannon", "InvSimpson","Chao1"), color="Elevation")
```
We can also plot beta diversity using NMDS. First we transform the data to be relative abundance

```{r}
ps.prop <- transform_sample_counts(ps_no_control, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
ord.nmds.bray

plot_ordination(ps.prop, ord.nmds.bray, color="Elevation", title="Bray NMDS")
```

Next we can get an idea of what is making up the different communities using bar plots. More functionality of phyloseq can be found at https://joey711.github.io/phyloseq/index.html

```{r}
ps.phylum<-tax_glom(ps_no_control, taxrank = "Phylum")
top10 <- names(sort(taxa_sums(ps.phylum), decreasing=TRUE))[1:10]
dat<-psmelt(ps.phylum)
dat<-mutate(dat, Phylum= ifelse(OTU %in% top10, Phylum, "Other"))

ggplot(dat, aes(x=Sample,y=Abundance, col=Phylum))+
  geom_col(aes( fill=Phylum), position="stack")+
  facet_grid(col=vars(Elevation), space="free_x",scales="free")+
  theme(axis.text.x =element_text(angle=90))
groupRA<-dat%>%
  group_by(Sample) %>%
  mutate(gAbun= sum(Abundance)) %>%
  group_by(Phylum, .add=TRUE) %>%
  mutate(RA=Abundance/gAbun)

ggplot(groupRA,aes(x=Sample, y=RA, fill=Phylum))+
  geom_col(position="stack")+
  facet_grid(cols  = vars(Elevation),space="free_x",scales="free_x")+
  theme(axis.text.x = element_text(angle=90))
```
